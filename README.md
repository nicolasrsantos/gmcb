This repository is the official PyTorch implementation of "Semi-Supervised Coarsening of Bipartite Graphs for Text Classification via Graph Neural Network" published in the 2024 IEEE 11th International Conference on Data Science and Advanced Analytics (DSAA).

## Requirements

This code was implemented using Python 3.11.5, CUDA 12.2 and the following packages:

- `networkx==3.3`
- `nltk==3.8.1`
- `numpy==1.26.4`
- `python_igraph==0.11.5`
- `PyYAML==6.0.1`
- `scikit_learn==1.4.2`
- `scipy==1.13.1`
- `torch==2.1.2`
- `torch_geometric==2.5.3`
- `python_igraph==0.11.5`

## How to run the code

In order to run our method, you must perform the steps described below. Notice that we have .sh scripts available to run the code as well.

### Obtaining GloVe's embeddings

Since our method uses GloVe's embeddings, you **must** create a directory called *embeddings* in the project's root directory. Then, download the embeddings (glove.6B.zip) from this url https://nlp.stanford.edu/projects/glove/ and extract the 300d file (glove.6B.300d.txt) to the directory you created.

### Build graph

To build a graph you can simply run the following command:

    $ python build_graph.py --dataset <dataset_name>

After running this command, a new directory called *graphs* will be created in the *data* directory. Inside the *graphs* directory you will find many files, including the edge index and the masks required by torch geometric, a file containg the documents' classes (.y), the embeddings for each word (.x_word) and document (.x_doc), and a map used by our method.

This repository contain a few datasets available in the *data* directory, including the datasets we used in the experiments we ran.

### Coarsening

After building a graph, you can coarsen it using the MFBN framework, which is available inside the *mfbn* directory. In order to do so, you **must** first run the following command to generate the .ncol file required by the mfbn.

    $ python build_coarse_ncol.py --dataset <dataset_name>

This command will generate a .ncol file and move it to the *input* directory inside the *mfbn_coarsening* folder. After this step, you have to copy the x_doc and x_word files to the same directory. Then, you can finally coarsen the graph by running the run_coarse.sh script inside the coarsening folder.

An example of the configuration file is available in mfbn's input folder. The most important parameters you must modify are the vertices, reduction_factor, and max_levels. Vertices is used to specify the number of nodes in each partition of the graph, reduction_factor controls the amount (in %) of contraction applied to each partition, and max_levels controls the number of contractions applied to each partition. For example, if max_levels = [10, 10], each partition will be coarsened hierarchically 10 times. For more information on mfbn's config file, check https://github.com/alanvalejo/mfbn

After coarsening the input graph, move the files generated by mfbn from the output directory to the data/graphs/ directory. Then, run the command below to parse the output files and adequate them to the input expected by torch geometric.

    $ python parse_coarse.py --dataset <dataset_name> --max_level <mfbn_max_level>

### GNN

Finally, you can train the GNN to perform text classification by running the following command:

    $ python train.py --dataset <dataset_name> --out_dim <number_of_classes>

The following arguments allow the modification of the GNN's hyperparameters:

- `--lr`

    Modifies the model's learning rate.
  
    Default: `1e-3`

- `--batch_size`

    Controls our method's batch size.
  
    Default: `32`

- `--hidden_dim`

    Number of dimensions used on GraphSAGE. 

    Default: `256`

- `--n_epochs`

    Number of training epochs.

    Default: `200`

- `--patience`

    Number of epochs without validation loss before early stopping.

    Default: `10`

- `--epoch_log`

    Prints information about the network's training every <epoch_log> steps.

    Default: `10`

- `--gpu`

    Trains the network using a GPU (if available).

    Default: `true`

- `--cpu`

    Trains the network using the CPU.

    Default: `false`

## Reference

```
@InProceedings{santos2024gmcb,
  author={dos Santos, Nícolas Roque and Minatel, Diego and Valejo, Alan Demétrius Baria and de Andrade Lopes, Alneu},
  booktitle={2024 IEEE 11th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Semi-Supervised Coarsening of Bipartite Graphs for Text Classification via Graph Neural Network}, 
  year={2024},
  pages={1-10},
  doi={10.1109/DSAA61799.2024.10722822}
}

```

## Acknowledgements

The Multilevel framework for bipartite networks (MFBN) plays an important role in our method. We adapted its original implementation to coarsen a graph using the cosine similarity of the words present in the datasets we used. For more information on MFBN, you can check the following repository: https://github.com/alanvalejo/mfbn.
